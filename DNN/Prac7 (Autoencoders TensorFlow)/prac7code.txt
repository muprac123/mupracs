import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import seaborn as sns

# Set random seeds for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

print("TensorFlow version:", tf.__version__)

# Load and preprocess MNIST dataset
print("Loading MNIST dataset...")
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(f"Training data shape: {x_train.shape}")
print(f"Test data shape: {x_test.shape}")

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten images for basic autoencoder
x_train_flat = x_train.reshape(-1, 28 * 28)
x_test_flat = x_test.reshape(-1, 28 * 28)

print(f"Flattened training data shape: {x_train_flat.shape}")


---
# =============================================================================
# 1. BASIC DENSE AUTOENCODER
# =============================================================================

print("\n" + "="*60)
print("BUILDING BASIC DENSE AUTOENCODER")
print("="*60)

# Hyperparameters
input_dim = 784  # 28x28
encoding_dim = 64  # Compressed representation size
epochs = 50
batch_size = 256

# Build encoder
encoder_input = layers.Input(shape=(input_dim,))
encoded = layers.Dense(128, activation='relu')(encoder_input)
encoded = layers.Dense(encoding_dim, activation='relu')(encoded)

# Build decoder
decoded = layers.Dense(128, activation='relu')(encoded)
decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)

# Create autoencoder model
basic_autoencoder = keras.Model(encoder_input, decoded)

# Compile model
basic_autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])

print("Basic Autoencoder Architecture:")
basic_autoencoder.summary()

# Train basic autoencoder
print("Training Basic Autoencoder...")
basic_history = basic_autoencoder.fit(
    x_train_flat, x_train_flat,  # Input and target are the same
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True,
    validation_data=(x_test_flat, x_test_flat),
    verbose=1
)

# Create separate encoder model for visualization
encoder_model = keras.Model(encoder_input, encoded)


---
# =============================================================================
# 2. CONVOLUTIONAL AUTOENCODER
# =============================================================================

print("\n" + "="*60)
print("BUILDING CONVOLUTIONAL AUTOENCODER")
print("="*60)

# Reshape data for CNN (add channel dimension)
x_train_cnn = x_train.reshape(-1, 28, 28, 1)
x_test_cnn = x_test.reshape(-1, 28, 28, 1)

# Build convolutional encoder
conv_input = layers.Input(shape=(28, 28, 1))

# Encoder
conv_encoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv_input)
conv_encoded = layers.MaxPooling2D((2, 2), padding='same')(conv_encoded)
conv_encoded = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(conv_encoded)
conv_encoded = layers.MaxPooling2D((2, 2), padding='same')(conv_encoded)
conv_encoded = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(conv_encoded)
conv_encoded = layers.MaxPooling2D((2, 2), padding='same')(conv_encoded)

# Decoder
conv_decoded = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(conv_encoded)
conv_decoded = layers.UpSampling2D((2, 2))(conv_decoded)
conv_decoded = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(conv_decoded)
conv_decoded = layers.UpSampling2D((2, 2))(conv_decoded)
conv_decoded = layers.Conv2D(32, (3, 3), activation='relu')(conv_decoded)
conv_decoded = layers.UpSampling2D((2, 2))(conv_decoded)
conv_decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv_decoded)

# Create convolutional autoencoder
conv_autoencoder = keras.Model(conv_input, conv_decoded)

# Compile model
conv_autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])

print("Convolutional Autoencoder Architecture:")
conv_autoencoder.summary()

# Train convolutional autoencoder
print("Training Convolutional Autoencoder...")
conv_history = conv_autoencoder.fit(
    x_train_cnn, x_train_cnn,
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True,
    validation_data=(x_test_cnn, x_test_cnn),
    verbose=1
)


---
# =============================================================================
# 3. DENOISING AUTOENCODER
# =============================================================================

print("\n" + "="*60)
print("BUILDING DENOISING AUTOENCODER")
print("="*60)

# Add noise to the data
noise_factor = 0.5
x_train_noisy = x_train_flat + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_flat.shape)
x_test_noisy = x_test_flat + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_flat.shape)

# Clip values to [0, 1]
x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

# Build denoising autoencoder (same architecture as basic)
denoise_input = layers.Input(shape=(input_dim,))
denoise_encoded = layers.Dense(128, activation='relu')(denoise_input)
denoise_encoded = layers.Dense(encoding_dim, activation='relu')(denoise_encoded)
denoise_decoded = layers.Dense(128, activation='relu')(denoise_encoded)
denoise_decoded = layers.Dense(input_dim, activation='sigmoid')(denoise_decoded)

denoising_autoencoder = keras.Model(denoise_input, denoise_decoded)

# Compile model
denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])

print("Denoising Autoencoder Architecture:")
denoising_autoencoder.summary()

# Train denoising autoencoder (input: noisy, target: clean)
print("Training Denoising Autoencoder...")
denoise_history = denoising_autoencoder.fit(
    x_train_noisy, x_train_flat,  # Input: noisy, Target: clean
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True,
    validation_data=(x_test_noisy, x_test_flat),
    verbose=1
)


---
# =============================================================================
# EVALUATION AND VISUALIZATION
# =============================================================================

print("\n" + "="*60)
print("EVALUATING MODELS")
print("="*60)

# Make predictions
basic_predictions = basic_autoencoder.predict(x_test_flat[:10])
conv_predictions = conv_autoencoder.predict(x_test_cnn[:10])
denoise_predictions = denoising_autoencoder.predict(x_test_noisy[:10])


# Calculate reconstruction errors
basic_mse = mean_squared_error(x_test_flat[:1000].flatten(),
                              basic_autoencoder.predict(x_test_flat[:1000]).flatten())
conv_mse = mean_squared_error(x_test_cnn[:1000].flatten(),
                             conv_autoencoder.predict(x_test_cnn[:1000]).flatten())
denoise_mse = mean_squared_error(x_test_flat[:1000].flatten(),
                                denoising_autoencoder.predict(x_test_noisy[:1000]).flatten())


print(f"Basic Autoencoder MSE: {basic_mse:.6f}")
print(f"Convolutional Autoencoder MSE: {conv_mse:.6f}")
print(f"Denoising Autoencoder MSE: {denoise_mse:.6f}")

# Plot training histories
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Basic Autoencoder
axes[0,0].plot(basic_history.history['loss'], label='Training Loss')
axes[0,0].plot(basic_history.history['val_loss'], label='Validation Loss')
axes[0,0].set_title('Basic Autoencoder Training')
axes[0,0].set_xlabel('Epoch')
axes[0,0].set_ylabel('Loss')
axes[0,0].legend()
axes[0,0].grid(True)

# Convolutional Autoencoder
axes[0,1].plot(conv_history.history['loss'], label='Training Loss')
axes[0,1].plot(conv_history.history['val_loss'], label='Validation Loss')
axes[0,1].set_title('Convolutional Autoencoder Training')
axes[0,1].set_xlabel('Epoch')
axes[0,1].set_ylabel('Loss')
axes[0,1].legend()
axes[0,1].grid(True)

# Denoising Autoencoder
axes[1,0].plot(denoise_history.history['loss'], label='Training Loss')
axes[1,0].plot(denoise_history.history['val_loss'], label='Validation Loss')
axes[1,0].set_title('Denoising Autoencoder Training')
axes[1,0].set_xlabel('Epoch')
axes[1,0].set_ylabel('Loss')
axes[1,0].legend()
axes[1,0].grid(True)



plt.tight_layout()
plt.show()

# Visualize reconstructions
fig, axes = plt.subplots(5, 10, figsize=(20, 10))

for i in range(10):
    # Original images
    axes[0, i].imshow(x_test[i], cmap='gray')
    axes[0, i].set_title('Original')
    axes[0, i].axis('off')

    # Basic autoencoder
    axes[1, i].imshow(basic_predictions[i].reshape(28, 28), cmap='gray')
    axes[1, i].set_title('Basic AE')
    axes[1, i].axis('off')

    # Convolutional autoencoder
    axes[2, i].imshow(conv_predictions[i].reshape(28, 28), cmap='gray')
    axes[2, i].set_title('Conv AE')
    axes[2, i].axis('off')

    # Denoising autoencoder (show noisy input and denoised output)
    axes[3, i].imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')
    axes[3, i].set_title('Noisy Input')
    axes[3, i].axis('off')

    # Denoising output
    axes[4, i].imshow(denoise_predictions[i].reshape(28, 28), cmap='gray')
    axes[4, i].set_title('Denoised')
    axes[4, i].axis('off')

plt.suptitle('Autoencoder Reconstructions Comparison', fontsize=16)
plt.tight_layout()
plt.show()

# Visualize latent space representations (for basic autoencoder)
print("Visualizing latent space representations...")

# Encode test data
encoded_imgs = encoder_model.predict(x_test_flat)

# Plot latent space (first 2 dimensions)
plt.figure(figsize=(12, 10))
scatter = plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test, cmap='tab10', alpha=0.7)
plt.colorbar(scatter)
plt.title('Latent Space Representation (First 2 Dimensions)')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')
plt.show()


---